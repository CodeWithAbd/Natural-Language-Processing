{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\abdwo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abdwo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1581820\n",
      "[('NN', ':'), (':', 'CD'), ('CD', 'NN'), ('NN', 'NNS'), ('NNS', 'VBP'), ('VBP', 'TO'), ('TO', 'DT'), ('DT', 'NN'), ('NN', 'NN'), ('NN', ','), (',', 'NN'), ('NN', 'CC'), ('CC', 'RB'), ('RB', 'NN'), ('NN', '.'), ('.', 'PRP'), ('PRP', 'VBP'), ('VBP', 'IN'), ('IN', 'DT'), ('DT', 'NN'), ('NN', '.'), ('.', 'CD'), ('CD', 'IN'), ('IN', 'DT'), ('DT', 'NN'), ('NN', 'VBZ'), ('VBZ', ','), (',', 'CC'), ('CC', 'PRP$'), ('PRP$', 'NN'), ('NN', 'VBZ'), ('VBZ', 'TO'), ('TO', 'VB'), ('VB', 'PRP'), ('PRP', 'IN'), ('IN', 'PRP$'), ('PRP$', 'NN'), ('NN', ','), (',', 'CC'), ('CC', 'VBZ'), ('VBZ', 'NNS'), ('NNS', '.'), ('.', 'WP'), ('WP', \"''\"), (\"''\", 'VBD'), ('VBD', 'DT'), ('DT', 'NN'), ('NN', '.'), ('.', 'VB'), ('VB', 'DT')]\n",
      "Frequency of pattern 'JJ NN': 0\n",
      "First 50 matched samples:\n",
      "fuck movie\n",
      "teen generation\n",
      "cool idea\n",
      "bad package\n",
      "t snag\n",
      "neat concept\n",
      "main problem\n",
      "\" fantasy\n",
      "t mind\n",
      "same clue\n",
      "big secret\n",
      "sad part\n",
      "little bit\n",
      "bottom line\n",
      "secret password\n",
      "melissa sagemiller\n",
      "further insight\n",
      "pretty decent\n",
      "fuck movie\n",
      "i guess\n",
      "little edge\n",
      "wes bentley\n",
      "same character\n",
      "american beauty\n",
      "new neighborhood\n",
      "entire film\n",
      "s unraveling\n",
      "t stick\n",
      "s confusing\n",
      "pretty redundant\n",
      "pretty cool\n",
      "s joblo\n",
      "elm street\n",
      "happy bastard\n",
      "quick movie\n",
      "head start\n",
      "russian tech\n",
      "few action\n",
      "big pink\n",
      "flashy thing\n",
      "donald sutherland\n",
      "s chase\n",
      "below average\n",
      "halloween h20\n",
      "real star\n",
      "stan winston\n",
      "s robot\n",
      "schnazzy cgi\n",
      "good gore\n",
      "s brain\n",
      "67830\n",
      "67830\n",
      "Frequency of pattern 'JJ NN': 0\n",
      "First 50 matched samples:\n",
      "fuck movie: JJ NN\n",
      "teen generation: JJ NN\n",
      "cool idea: JJ NN\n",
      "bad package: JJ NN\n",
      "t snag: JJ NN\n",
      "neat concept: JJ NN\n",
      "main problem: JJ NN\n",
      "\" fantasy: JJ NN\n",
      "t mind: JJ NN\n",
      "same clue: JJ NN\n",
      "big secret: JJ NN\n",
      "sad part: JJ NN\n",
      "little bit: JJ NN\n",
      "bottom line: JJ NN\n",
      "secret password: JJ NN\n",
      "melissa sagemiller: JJ NN\n",
      "further insight: JJ NN\n",
      "pretty decent: JJ NN\n",
      "fuck movie: JJ NN\n",
      "i guess: JJ NN\n",
      "little edge: JJ NN\n",
      "wes bentley: JJ NN\n",
      "same character: JJ NN\n",
      "american beauty: JJ NN\n",
      "new neighborhood: JJ NN\n",
      "entire film: JJ NN\n",
      "s unraveling: JJ NN\n",
      "t stick: JJ NN\n",
      "s confusing: JJ NN\n",
      "pretty redundant: JJ NN\n",
      "pretty cool: JJ NN\n",
      "s joblo: JJ NN\n",
      "elm street: JJ NN\n",
      "happy bastard: JJ NN\n",
      "quick movie: JJ NN\n",
      "head start: JJ NN\n",
      "russian tech: JJ NN\n",
      "few action: JJ NN\n",
      "big pink: JJ NN\n",
      "flashy thing: JJ NN\n",
      "donald sutherland: JJ NN\n",
      "s chase: JJ NN\n",
      "below average: JJ NN\n",
      "halloween h20: JJ NN\n",
      "real star: JJ NN\n",
      "stan winston: JJ NN\n",
      "s robot: JJ NN\n",
      "schnazzy cgi: JJ NN\n",
      "good gore: JJ NN\n",
      "s brain: JJ NN\n"
     ]
    }
   ],
   "source": [
    "# in this example I used Ngrams functions from nltk library\n",
    "# the function is used to perform the N-Grams based POS-Tag analysis at Movie Reviews Data-Set.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import ngrams\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# define the target pattern and the size of the n-grams\n",
    "pattern = \"JJ NN\"\n",
    "n = 2\n",
    "\n",
    "# create the POS tag N-grams of the movie reviews\n",
    "pos_ngrams = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    words = movie_reviews.words(fileid)\n",
    "    tags = [tag for word, tag in nltk.pos_tag(words)]\n",
    "    pos_ngrams += ngrams(tags, n)\n",
    "print(len(pos_ngrams))\n",
    "print(pos_ngrams[0:50])\n",
    "\n",
    "# find the samples that match the target pattern\n",
    "matched_samples = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    words = movie_reviews.words(fileid)\n",
    "    tags = [tag for word, tag in nltk.pos_tag(words)]\n",
    "    tag_ngrams = ngrams(tags, n)\n",
    "    for i, gram in enumerate(tag_ngrams):\n",
    "        if \" \".join(gram) == pattern:\n",
    "            matched_samples.append(\" \".join(words[i:i+n]))\n",
    "\n",
    "# print the frequency and the first 50 matched samples of the target pattern\n",
    "print(\"Frequency of pattern '{}': {}\".format(pattern, pos_ngrams.count(pattern.split())))\n",
    "print(\"First 50 matched samples:\")\n",
    "for sample in matched_samples[:50]:\n",
    "    print(sample)\n",
    "\n",
    "# find the samples that match the target pattern\n",
    "matched_samples = []\n",
    "matched_pos_tags = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    words = movie_reviews.words(fileid)\n",
    "    tags = [tag for word, tag in nltk.pos_tag(words)]\n",
    "    tag_ngrams = ngrams(tags, n)\n",
    "    for i, gram in enumerate(tag_ngrams):\n",
    "        if \" \".join(gram) == pattern:\n",
    "            matched_samples.append(\" \".join(words[i:i+n]))\n",
    "            matched_pos_tags.append(\" \".join(tags[i:i+n]))\n",
    "print(len(matched_samples))\n",
    "print(len(matched_pos_tags))\n",
    "\n",
    "# print the frequency and the first 50 matched samples of the target pattern\n",
    "print(\"Frequency of pattern '{}': {}\".format(pattern, pos_ngrams.count(pattern.split())))\n",
    "print(\"First 50 matched samples:\")\n",
    "for i in range(50):\n",
    "    print(\"{}: {}\".format(matched_samples[i], matched_pos_tags[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the Frequncy of a specific POS tag pattern\n",
    "## Using N-Grams in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RB', 'IN', 'JJ', 'NNP', 'VBD', 'PRP', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'PRP', 'VBD', 'DT', 'NNS', 'IN', 'NN', 'CC', 'RB', 'PRP', 'VBD', 'TO', 'VBG', 'IN', 'PRP$', 'NNS', 'IN', 'DT', 'NN', 'CD', 'JJ', 'JJR', 'NN', 'IN', 'DT', 'NN', 'PRP', 'VBD', 'RB', 'JJ', 'CD', 'NN', 'NN', 'DT', 'VBD', 'DT', 'JJ']\n",
      "11\n",
      "Frequency of pattern 'DT NN': 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abdwo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Here I am counting the frequency of a specific POS tag pattern\n",
    "# using N-grams in Python:\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import *\n",
    "\n",
    "\n",
    "download('averaged_perceptron_tagger')\n",
    "\n",
    "# define the text and the target pattern\n",
    "text = \"\"\"Once while traveling, Raman found himself in the company of a group of soldiers.\n",
    "\n",
    "They were all veterans of war and soon they got to talking about their experiences on the battlefield. One old soldier told of the time he had single-handedly slain seven enemy soldiers. Another gave a detailed description of the manner in which he had held an entire enemy battalion at bay.\n",
    "\n",
    "When they had finished they looked condescendingly at Raman.\n",
    "\n",
    "\"I don't suppose you have any adventure worth telling,\" said one of the grizzled warriors.\n",
    "\n",
    "\"Oh, but I have,\" said Rama\n",
    "\n",
    "\"You have?!\" said the soldiers.\n",
    "\n",
    "\"Yes,\" said Rama. \"Once while traveling I chanced upon a large tent. I entered and there, lying on a mat was the largest man I had ever seen. I recognized him at once as a dreaded dacoit who had been terrorizing that part of the country for years!\"\n",
    "\n",
    "\"What did you do?\" asked the soldiers, their interest now fully aroused.\n",
    "\n",
    "\"I cut off his toe and ran for dear life,\" said Rama.\n",
    "\n",
    "\"His toe?\" said a soldier. \"Why toe? You should have cut off his head while you had the chance!\"\n",
    "\n",
    "\"Somebody had already done that,\" said Rama, grinning.\"\"\"\n",
    "\n",
    "pattern = \"DT NN\"\n",
    "\n",
    "# define the size of the n-grams\n",
    "n = 2\n",
    "\n",
    "# create the POS tags of the text\n",
    "pos_tags = [tag for word, tag in pos_tag(text.split())]\n",
    "print(pos_tags[:50])\n",
    "\n",
    "# create the N-grams of the POS tags\n",
    "pos_ngrams = ngrams(pos_tags, n)\n",
    "\n",
    "# count the frequency of the target pattern\n",
    "pos_ngram_counter = Counter([\" \".join(gram) for gram in pos_ngrams])\n",
    "pattern_freq = pos_ngram_counter[pattern]\n",
    "print(pattern_freq)\n",
    "\n",
    "# print the frequency of the target pattern\n",
    "print(\"Frequency of pattern '{}': {}\".format(pattern, pattern_freq))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uni-grams, Bi-grams, Tri-grams without using the string manipulation and loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['brown fox', 'by the', 'fox jumps', 'how much', 'jumps over',\n",
       "       'lazy dog', 'lorry red', 'lorry yellow', 'much wood', 'over the',\n",
       "       'quick brown', 'red lorry', 'seashells by', 'sells seashells',\n",
       "       'she sells', 'the lazy', 'the quick', 'the seashore', 'wood would',\n",
       "       'woodchuck chuck', 'would woodchuck', 'yellow lorry'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I created this file to create he uni-grams, big-rams and tri-grams without using the strings manipulation and loops\n",
    "# No any library like NLTK is not used.\n",
    "# At last countVectorizer is used to generate ngrams features\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "words = text.split()\n",
    "words\n",
    "\n",
    "# create bigrams\n",
    "bigrams = []\n",
    "for i in range(len(words) - 1):\n",
    "    bigrams.append(words[i] + \" \" + words[i + 1])\n",
    "bigrams\n",
    "\n",
    "# create trigrams\n",
    "trigrams = []\n",
    "for i in range(len(words) - 2):\n",
    "    trigrams.append(words[i] + \" \" + words[i + 1] + \" \" + words[i + 2])\n",
    "trigrams\n",
    "\n",
    "# create bag of words representations\n",
    "bow_bigrams = {}\n",
    "for bigram in bigrams:\n",
    "    if bigram in bow_bigrams:\n",
    "        bow_bigrams[bigram] += 1\n",
    "    else:\n",
    "        bow_bigrams[bigram] = 1\n",
    "bow_bigrams\n",
    "\n",
    "bow_trigrams = {}\n",
    "for trigram in trigrams:\n",
    "    if trigram in bow_trigrams:\n",
    "        bow_trigrams[trigram] += 1\n",
    "    else:\n",
    "        bow_trigrams[trigram] = 1\n",
    "bow_trigrams\n",
    "\n",
    "print(len(words))\n",
    "print(len(bigrams))\n",
    "print(len(trigrams))\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# example data\n",
    "texts = [\"The quick brown fox jumps over the lazy dog\",\n",
    "         \"She sells seashells by the seashore\",\n",
    "         \"How much wood would a woodchuck chuck\",\n",
    "         \"Red lorry, yellow lorry, red lorry, yellow lorry\"]\n",
    "\n",
    "labels = [\"animals\", \"beach\", \"wood\", \"vehicles\"]\n",
    "\n",
    "# create bigrams from the texts\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "X\n",
    "\n",
    "X.toarray()[1]\n",
    "\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
